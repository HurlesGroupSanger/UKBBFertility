---
title: "SNV Calling and Filtering"
author: "Eugene Gardner"
date: "03 December 2020"
output: 
  html_document:
    toc: true
    toc_depth: 2
---

# 1. Startup and Introduction

This Markdown documents the annotation and extraction of SNV and InDel variants from UKBB FE WES files. This annotation uses a custom java pipeline, of which the precompiled jar is included at `scripts/SNVCounter.jar`. The source code and Eclipse project for this pipeline is available in `src/SNVCounter/`.

This code makes use of the following tools:

1. plinkv1.9/2 to extract variants from primary UKBB data: https://www.cog-genomics.org/plink2
2. CADD requires a clone and install of their code repo: https://github.com/kircherlab/CADD-scripts/
3. VEP requires a clone and install of their code repo: https://github.com/Ensembl/ensembl-vep
    * Below I provide a rough set of instructions to install [LOFTEE](https://github.com/konradjk/loftee) by Konrad Karczewski from gnomAD, which can be _very_ annoying for hg38
4. To liftover MPC and PEXT to hg38, we need [CrossMap](http://crossmap.sourceforge.net/)
5. As a supplement to crossmap, a local download of the GRCh38 reference to check that our mapping is correct
6. [bcftools](http://samtools.github.io/bcftools/bcftools.html) is used in many places to parse information out of downloaded vcf/bcf files.
    * The bcftools plugin [split-vep](https://samtools.github.io/bcftools/howtos/plugin.split-vep.html) is also used when parsing vep output. Information on how to use plugins with bcftools is available [here](https://samtools.github.io/bcftools/howtos/index.html).
7. bgzip and tabix, which is provided as part of [htslib](https://github.com/samtools/htslib).
8. bedtools, which is used for various genome arithmatic functions

**Note**: The intent of this document is _not_ to be a runnable document like `PhenotypeTesting.Rmd`, it is instead to offer a guide on how SNV/InDel annotation and curation was performed. We have provided the output of this document to UK Biobank, which you can download at:

`https://fake.ukbbiobank_url.com`

You can view a compiled html version of this document with all code run either within this repository at `compiled_htmls/SNVCalling_Filtering.html` or on [github](https://htmlpreview.github.io/?https://github.com/eugenegardner/UKBBFertility/blob/master/compiled_html/SNVCalling_Filtering.html).

# 2. Required Exome Variant Files

First step to analyse UK Biobank WES data is to actually acquire the variant files themselves. Please see [this page](http://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=23160) for additional information on accquiring required files. Please note that conversion to VCF format requires [PLINKv1.9 or PLINKv2](https://www.cog-genomics.org/plink2) and [tabix](http://www.htslib.org/doc/tabix.html). 

**Note**: DO NOT delete the raw files after you download them. VEP annotation requires them to work properly!

Roughly, files were accquired and processed into a VCF format file with the following commands:

```{bash Get UKBB VCF, eval = F}

## Going to put these files in vcfs/ to make examples in this document consistent:
mkdir vcfs/
cd vcfs/

## This downloads a set of plink binary format files: ukb_efe_chr1_v1.bed, ukb44165_efe_chr1_v1_s49959.fam, ukb_fe_exm_chrall_v1.bim
## Note that you must also have your keyfile provided by UKB pointed to with the -a flag
ukbgene efe -c1 ## bed file
ukbgene efe -c1 -m ## fam file
curl -O biobank.ndph.ox.ac.uk/showcase/showcase/auxdata/ukb_fe_exm_chrall_v1.bim ## bim file

## This converts downloaded plink files to a VCF format file useable by this code:
plink --bed ukb_efe_chr1_v1.bed --bim ukb_fe_exm_chrall_v1.bim --fam ukb44165_efe_chr1_v1_s49959.fam --out ukbb_plink --recode vcf

## And bgzip and index:
bgzip ukbb_plink.vcf
tabix -p vcf ukbb_plink.vcf.gz
```

# 3. Generating Annotation Sources

This process is slightly complicated because I have written this code to be project-agnostic and therefore makes the possible sources of data that I am accounting for more complicated. I have provided here a simplified version of this annotation for Hg38 UK Biobank data, but this code should be fairly adaptable to other sequencing formats.

**Note**: All annotations below are done *without* the "chr" prefix!

Basic Steps:

1. Variant Agnostic Databases -- For each of the below annotations I need to generate GRCh38 versions. Annotations are stored in the root folder `annotations/hg38/` with the corresponding directory name in brackets below:
    *CADD scores [`cadd/`]
        + We also have to generate CADD scores for variants not included in the default build.
        + I believe this is completely limited to InDels.
    * gnomAD allele frequencies [`gnomad/`]
        + Note that I am using AF_nfe, which is non-Finnish European AF
    * MPC scores [`mpc/`]
        + Have to lift this over to hg38
    * PEXT scores [`pext/`]
        + Have to lift this over to hg38
2. Variant Specific Databases -- Need to generate annotations for each VCF as variant loci/values will not be identical. These annotations are stored alongside the vcf file, with a changed suffix like `ukbb_plink.*`, where * is the name of the information within:
    * VQSR annotations [`ukbb_plink.vqsr/`]
        + For the purposes of UKBB this amounts to a 'fake' list of VQSR values for each variant as we "assume" that these variants were already filtered to a high sensitivity/specificity. This just ensures that all variants pass default VQSR filtering in the java pipeline
    * VEP annotations [`ukbb_plink.vep/`]
    * Supplemental CADD annotations [`ukbb_plink.cadd/`]

All of the basic annotation is being done in root directory. As noted above, per-vcf annotations are located in `./vcfs/`. The actual "mashing together" of annotations will be performed by a [java pipeline](#3.-running-the-variant-engine).

**Note**: _Many_ of the below commands will take a long time, either due to long download times or to actual processing time. It is highly recommended to run these with some sort of parallel compute architecture. Nonetheless, all of these commands should be runnable on a local machine.

## 3A. Create Required Directory Structure

```{bash Create Directories, eval = F}

## Make top level directories:
mkdir -p annotations/hg38/

## Make annotation-specific directories:
mkdir annotations/hg38/cadd/
mkdir annotations/hg38/gnomad/
mkdir annotations/hg38/mpc/
mkdir annotations/hg38/pext/

## Make variant-specific directories:
mkdir vcfs/ukbb_plink.cadd/
mkdir vcfs/ukbb_plink.vep/
mkdir vcfs/ukbb_plink.vqsr/
```

## 3B. Variant Agnostic Databases

### CADD

Here we download v1.5 of CADD. At the time of generating the data used in this study, this is what was available. As of writing this document, v1.6 has been replaced and should be easy to substitute here (the file formats are the same). There is no necessary postprocessing required for these files. Simply download, ensure the name is consistent with what is below and you are good to go:

```{bash Obtain CADD Annotations, eval = F}
## Go to CADD directory:
cd annotations/hg38/cadd/

# Download CADD Table and index
curl -o annotations/hg38/cadd/whole_genome_SNVs.tsv.gz http://krishna.gs.washington.edu/download/CADD/v1.5/GRCh38/whole_genome_SNVs.tsv.gz
curl -o annotations/hg38/cadd/whole_genome_SNVs.tsv.gz.tbi http://krishna.gs.washington.edu/download/CADD/v1.5/GRCh38/whole_genome_SNVs.tsv.gz.tbi
```

### gnomAD

This code has two ways to function, depending on if you have the gnomAD *v3* sites vcf as a local download. Thus, you can either use:

1. a web-enabled version of bcftools to stream sites (slower, but no storage needed).
2. or you can download your own local version (can be used for other projects).
    * Make sure you download the .tbi index as well!

gnomAD v3 sites are available here: `https://storage.googleapis.com/gnomad-public/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz`

```{bash Obtain Gnomad Annotations, eval = F}
## Go to gnomAD directory:
cd annotations/hg38/gnomad/

# Parse w/BCFtools:
## Online version:
bcftools query https://storage.googleapis.com/gnomad-public/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz -f "%CHROM\t%POS\t%REF\t%ALT\t%FILTER\t%INFO/AF_nfe\n" > gnomad.tsv

## Offline version:
## Obviously change the path to your local download:
bcftools query /path/to/gnomadv3/gnomad.genomes.r3.0.sites.vcf.bgz -f "%CHROM\t%POS\t%REF\t%ALT\t%FILTER\t%INFO/AF_nfe\n" > gnomad.tsv

## Remove 'chr' prefix, bgzip, and tabix index.
sed -i 's_chr__' gnomad.tsv
bgzip gnomad.tsv
tabix -s 1 -b 2 -e 2 gnomad.tsv.gz
```

### MPC

Obtaining MPC scores from the [Samocha et al.](https://www.biorxiv.org/content/10.1101/148353v1), which developed a score for missense constraint. The table we use includes MPC values for all missense variants in the genome. As this file does not exist for hg38, we need to use crossmap to lift it over.

We hope that CrossMap is doing a good job, but we also do further checks as part of running our annotation engine to ensure that variants are being annotated properly.

```{bash Make hg38 MPC, eval = F}
## Go to MPC directory:
cd annotations/hg38/mpc/

## Get hg19 MPC:
curl -o fordist_constraint_official_mpc_values_v2.txt.gz https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release1/regional_missense_constraint/fordist_constraint_official_mpc_values_v2.txt.gz
curl -o fordist_constraint_official_mpc_values_v2.txt.gz.tbi https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release1/regional_missense_constraint/fordist_constraint_official_mpc_values_v2.txt.gz.tbi

## Download the hg19 -> hg38 chainfile from UCSC:
curl -o GRCh37_to_GRCh38.chain.gz ftp://ftp.ensembl.org/pub/assembly_mapping/homo_sapiens/GRCh37_to_GRCh38.chain.gz

## Need to make a fake VCF file for crossmap to use
# need VCF so that crossmap will check REF/ALT)
# Encode the original file as an INFO field so we can decode it later
# Make sure to add chr because liftover requires it (chain file and all...)
zcat fordist_constraint_official_mpc_values_v2.txt.gz | perl -ne 'chomp $_; @F = split("\t", $_); if ($F[0] =~ /chrom/) {print "#CHROM\tPOS\tID\tREF\tALT\tQUALT\tFILTER\tINFO\n";} else {print "chr$F[0]\t$F[1]\t.\t$F[2]\t$F[3]\t.\t.\tMPC=" . join("|",@F) . "\n"}' > mpc.hg19.vcf

## Run crossmap:
python /path/to/CrossMap.py vcf GRCh37_to_GRCh38.chain.gz mpc.hg19.vcf /path/to/Homo_sapiens.GRCh38_15.fa pext.hg38.vcf

## Convert back to the original file format:
grep -v '#' mpc.hg38.vcf | perl -ane 'chomp $_; $F[7] =~ s/MPC=//; @M = split("\\|", $F[7]); $M[0] = $F[0]; $M[1] = $F[1]; $M[2] = $F[3]; $M[3] = $F[4]; $M[0] =~ s/chr//; print join("\t", @M) . "\n";' > mpc.hg38.unsorted.tsv

## sort, zip, index
sort -k 1,1 -k 2,2n mpc.hg38.unsorted.tsv > mpc.tsv
bgzip mpc.tsv
tabix -s 1 -b 2 -e 2 mpc.tsv.gz
```

### PEXT

Obtaining PEXT scores from [Cummings et al.](https://www.biorxiv.org/content/10.1101/554444v2). This manuscript utilized data from GTEx to determine the proportion of transcripts in which a variant is located are actually expressed. Like with MPC, this file does not exist for hg38, and we need to use crossmap to lift it over. The protocol is thus very similar.

```{bash Make hg38 pext, eval=F}
## Go to PEXT directory:
cd annotations/hg38/pext/

## First get hg19 version files:
curl -o all.possible.snvs.tx_annotated.GTEx.v7.021520.tsv.bgz https://storage.googleapis.com/gnomad-public/papers/2019-tx-annotation/pre_computed/all.possible.snvs.tx_annotated.GTEx.v7.021520.tsv.bgz
curl -o all.possible.snvs.tx_annotated.GTEx.v7.021520.tsv.bgz.tbi https://storage.googleapis.com/gnomad-public/papers/2019-tx-annotation/pre_computed/all.possible.snvs.tx_annotated.GTEx.v7.021520.tsv.bgz.tbi

## Parse mean-pext using python script (easier w/python as per-tissue scores are stored as a python table/json):
# This takes a pretty long time to run, so a good idea to launch on parallel compute cluster
./scripts/format_pext.py all.possible.snvs.tx_annotated.021819.tsv.gz pext.tsv

## Download the hg19 -> hg38 chainfile from UCSC:
curl -o GRCh37_to_GRCh38.chain.gz ftp://ftp.ensembl.org/pub/assembly_mapping/homo_sapiens/GRCh37_to_GRCh38.chain.gz

## Need to make a fake VCF file for crossmap to use
# need VCF so that crossmap will check REF/ALT)
# Make sure to add chr because liftover requires it (chain file and all...)
# Also add header so that we can parse fake INFO fields we create, set to strings to keep proper format
perl -ane 'chomp $_; print "$F[0]\t$F[1]\t.\t$F[3]\t$F[4]\t.\t.\tAVG=$F[5];CER=$F[6];GENE=$F[7]\n";' pext.tsv > pext.vcf
echo -e "##fileformat=VCFv4.3\n##INFO=<ID=AVG,Number=1,Type=String,Description=\"AVG Expression\">\n##INFO=<ID=CER,Number=1,Type=String,Description=\"CER Expression\">\n##INFO=<ID=GENE,Number=1,Type=String,Description=\"Gene name\">\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO" | cat - pext.vcf > pext.hg19.vcf

## Run crossmap:
# Also takes a good amount of time to run
python /path/to/CrossMap.py vcf GRCh37_to_GRCh38.chain.gz pext.hg19.vcf /path/to/Homo_sapiens.GRCh38_15.fa pext.hg38.vcf
python ~/.local/bin/CrossMap.py vcf GRCh37_to_GRCh38.chain.gz pext.hg19.vcf /lustre/scratch115/resources/ref/Homo_sapiens/GRCh38_15/Homo_sapiens.GRCh38_15.fa pext.hg38.vcf

## Convert back to the original file format:
bcftools query -f "%CHROM\t%POS\t%REF\t%ALT\t%INFO/AVG\t%INFO/CER\t%INFO/GENE\n" pext.hg38.vcf > pext.hg38.unsorted.tsv

## sort, zip, index
sort -k 1,1 -k 2,2n pext.hg38.unsorted.tsv > pext.tsv 
bgzip pext.tsv
tabix -s 1 -b 2 -e 2 pext.tsv.gz
```

## 3C. Variant Specific Databases

### VQSR

As UK Biobank provided variants in plink format, VQSR values are not inclued. Due to this code also being needed for other projects, I do annotate VQSR for all variants. However, for UK Biobank calls, we just set a VQSR value of 10 (to make sure all variants pass even lenient filters).

```{bash Obtain VQSR Annotations, eval = F}
## Go to VQSR directory:
cd vcfs/ukbb_plink.vcf.vqsr/

## This is a FAKE file with all 10s for VQSR:
bcftools query -f "%CHROM\t%POS\t%REF\t%ALT\t%FILTER\t10\n" ../ukbb_plink.vcf.vcf.gz > VQSR.tsv
sed -i 's_chr__' VQSR.tsv
bgzip VQSR.tsv
tabix -s 1 -b 2 -e 2 VQSR.tsv.gz

```

### CADD

As the CADD library files that were created above do not contain most InDel scores, we need to supplement this file with a set of CADD scores for InDels in the primary plink file. This annotation requires a local install of [CADD](https://github.com/kircherlab/CADD-scripts/), the setup of which I do not include in this document. 

As CADD takes a good amount of time, this code is partially parallelized by the script `scripts/run_cadd.pl`.

```{bash run CADD supplemental annotations, eval = F}
## Go to CADD directory:
cd vcfs/ukbb_plink.vcf.cadd/

## Quicker to just pull the InDels out of our pre-processed 'VQSR' file above:
## This creates a master 'VCF' file of InDels that we need to annotate
zcat ../ukbb_plink.vqsr/VQSR.tsv.gz | perl -ane 'chomp $_; if (length($F[2]) > 1 || length($F[3]) > 1) {print "$F[0]\t$F[1]\t.\t$F[2]\t$F[3]\n";}' > to_annotate.vcf

## Just use split to create a set of files each with 1,000 lines that we can parellelize:
# For UKBB, this made a total of 765 files, but could change in the future
split -a 4 -l 1000 -d --numeric-suffixes=1 to_annotate.vcf to_annotate.

## Have to rename our files so CADD will like them
perl -e 'for (1..765) {$x = sprintf("%04d", $_); print "mv to_annotate.$x to_annotate.$x.vcf;\n";}' | bash

## This is provided for example, but CADD recommends an install via conda.
## Make sure you have it/activate it prior to running CADD via the provided script
conda activate cadd-env-1.5

## Run CADD
# Standard example, where the only input is the chunk we want to run, in this case a number from 1-765
scripts/run_cadd.pl <1-765>
# Example for LFS Submission
bsub -q normal -M 3000 -o CADD.%I.%J -J 'CADD[1-999]' './run_cadd.pl 1'

## Mash everything together:
perl -e '@F; for (1..765) {$x = sprintf("%04d",$_); push(@F, "to_annotate.$x.tsv.gz");} print "zcat " . join("\t", @F) . "\n";' | bash | grep -v "#" > cadd.tsv

## Sort, bgzip, and index
sort -k 1,1 -k2,2n cadd.tsv > cadd.sorted.tsv
mv cadd.sorted.tsv cadd.tsv 
bgzip cadd.tsv
tabix -s 1 -b 2 -e 2 cadd.tsv.gz
```

### VEP

For VEP, we use a build of v97, which can be obtained from github with the following commands:

```
git clone https://github.com/Ensembl/ensembl-vep.git
cd ensembl-vep/
git checkout release/97
```

New-ish builds should be OK, but have not been tested. VEP has to be run differently for each genome build (due to differences b/w the VEP cache/LOFTEE for each genome build). This is mostly handled by the provided script `scripts/make_chr_sites.pl`.

**Big Note**: This paths within this script MUST be edited to fit any local VEP build. 

I run VEP annotation in 322 chunks (dictated by bedtools makewindows), but the important part is the actual command run by `make_chr_sites.pl` to ensure formating is consistent with the downstream code.

This script requires:

1. VEP
2. bcftools & the bcftools plugin split-vep
3. plink

A **Note** on LOFTEE:

The most complicated part of getting VEP to work was building loftee for GRCh38. Ensure you have a plugins dir for VEP as described [here](https://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html). Some tips when doing this:

1. Pull the grch38 version of Konrad's [git repo](https://github.com/konradjk/loftee):
2. Download each of the required files [for hg38](https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/):
    * human ancestor
    * sql database
    * GERP database
3. For loftee Hg38 to run, it needs additional perl packages, most of which can easily be installed easily via cpan (not going to list here). The only one that was an issue was Bio::DB::BigWig, which has a whole custom setup to follow. I fixed this by installing an older version of the "Kent" UCSC repo [v334](https://github.com/ucscGenomeBrowser/kent/tree/v334_branch) as referenced in [this issue](https://github.com/GMOD/GBrowse-Adaptors/issues/17). This also requires you to edit a file when installing kent as noted in the same issue.

```{bash Run VEP, eval = F}

## Go to Vep directory
cd vcfs/ukbb_plink.vep/

# Generate FAI of first 24 chromosomes and chunks to run:
# This _should_ generate a file 322 lines long
head -24 /path/to/Homo_sapiens.GRCh38_15.fa.fai > GRCh38.fai
bedtools makewindows -g GRCh38.fai -w 10000000 > chunks.bed

# Actually run vep:
# This takes as input the line in the chunks.bed file that you want to annotate so it is parallelizable:
# So provide a number between 1 and 322 on the command line to annotate that particular chunk
./make_chr_sites.pl <1-322>

# Note - 12 of these chunks will generate an error when running plink which states: "All variants excluded". This is because these chunks contain ONLY heterochromatin and thus no variants. This error can be ignored. Other chunks may err out so make sure to check these!
# Had to write a custom script that uses the "chunks.bed" file to properly concatenate the chunks in order:
./cat.pl > vep.tsv
sed -i 's_chr__' vep.tsv
bgzip vep.tsv
tabix -s 1 -b 2 -e 2 vep.tsv.gz
```

# 4. Variant Engine

This next bit just runs the annotation engine I have generated in java. This annotation comes as both a compiled .jar file (compiled for Java14), as well as raw src files, which have additional minor documentation annotated within it. This source is located at `src/SNVCounter/` and comes with a pre-set Eclipse project `src/SNVCounter/.project` that can be imported into Eclipse for editing purposes.

## 4A. Preparing Gene Lists

This code block is almost _identical_ to the one that is the "Curating Genelists" section in the `PhenotypeTesting.Rmd` document. The only exception is that we create a 'coordinates.txt' file for both hg19 (not used for Hg38) and hg38 that is used as the primary genelist input for our annotation engine.

Reiterating what gene lists that we use here:

1. ENSEMBL-downloaded resources from [BioMart](https://www.ensembl.org/biomart/martview/0511514c231557b5d24ace4e8f7862e0).
2. pLI information from the [gnomAD project](https://storage.googleapis.com/gnomad-public/release/2.1.1/constraint/gnomad.v2.1.1.lof_metrics.by_gene.txt.bgz).
3. s~het~ from [Weghorn et al](https://doi.org/10.1093/molbev/msz092). The reference file is included in this repository (`raw_data/genelist/shet.weghorn.txt`).

**Note**: These scripts assume you have `curl` installed on your system, which _should_ be true if you are using macos and most *nix systems. Please change the scripts below if this is not the case.

### Download Resources from BioMart

```{r Generate biomart resources, warning=FALSE}

## Load packages silently
load.package <- function(name) {
  suppressMessages(suppressWarnings(library(name, quietly = T, warn.conflicts = F, character.only = T)))
}

## First and only place code is generated
load.package("biomaRt")
load.package("data.table")
load.package("tidyverse")

## Hg19
ensembl <- useMart("ensembl", host="http://grch37.ensembl.org", dataset = "hsapiens_gene_ensembl")
hg19.table <- data.table(getBM(attributes = c('ensembl_gene_id','chromosome_name','start_position','end_position','hgnc_id','hgnc_symbol','ensembl_transcript_id'),mart = ensembl))
hg19.table <- hg19.table[!grep("_",chromosome_name)]
write.table(hg19.table,"rawdata/genelists/hg19.genes.txt",col.names=F,row.names=F,quote=F,sep="\t")

## Hg38
ensembl <- useMart("ensembl", dataset = "hsapiens_gene_ensembl")
hg38.table <- data.table(getBM(attributes = c('ensembl_gene_id','chromosome_name','start_position','end_position','hgnc_id','hgnc_symbol','strand'),mart = ensembl))
hg38.table[,hgnc_id:=str_remove(hgnc_id,"HGNC:"),by=1:nrow(hg38.table)]
hg38.table <- hg38.table[!grep("CHR_",chromosome_name)]
write.table(hg38.table,"rawdata/genelists/hg38.genes.txt",col.names=F,row.names=F,quote=F,sep="\t")

rm(hg19.table,hg38.table,ensembl)
```

### s~het~ Gene Lists

```{bash Generate sHET gene lists}

perl -ane 'chomp $_; @F = split("\t", $_); print "$F[0]\t$F[7]\n";' rawdata/genelists/shet.weghorn.txt > rawdata/genelists/shet.processed.weghorn.txt

## sHET gene lists (have to attach ENSG):
scripts/matcher.pl -file1 rawdata/genelists/hg19.genes.txt -col1 5 -file2 rawdata/genelists/shet.processed.weghorn.txt -r | perl -ane 'chomp $_; print "$F[2]\t$F[0]\t$F[1]\t$F[6]\n";' > rawdata/genelists/shet.hgnc.txt
```

### Hg19 Gene Lists

```{bash Generate hg19 Gene Lists}

## Download gnomAD scores:
curl -o rawdata/genelists/gnomad.v2.1.1.lof_metrics.by_gene.txt.bgz https://storage.googleapis.com/gnomad-public/release/2.1.1/constraint/gnomad.v2.1.1.lof_metrics.by_gene.txt.bgz

## Rename your files gnomAD........
mv rawdata/genelists/gnomad.v2.1.1.lof_metrics.by_gene.txt.bgz rawdata/genelists/gnomad.v2.1.1.lof_metrics.by_gene.txt.gz
gunzip -f rawdata/genelists/gnomad.v2.1.1.lof_metrics.by_gene.txt.gz

## Create a reference file of just ENSG and pLI, while removing genes w/o a pLI score:
perl -ane 'chomp $_; @F = split("\t", $_); if ($F[20] ne 'NA') {print "$F[63]\t$F[20]\n";}' rawdata/genelists/gnomad.v2.1.1.lof_metrics.by_gene.txt > rawdata/genelists/hg19.all_genes_with_pli.txt

## Add additional info from biomart that we acquired:
# pLI file:
scripts/matcher.pl -file1 rawdata/genelists/hg19.genes.txt -file2 rawdata/genelists/hg19.all_genes_with_pli.txt -r | perl -ne 'chomp $_;  @F = split("\t", $_); print "$F[0]\t$F[3]\t$F[4]\t$F[5]\t$F[6]\t$F[7]\t$F[8]\t$F[1]\n";' > rawdata/genelists/hg19.all_genes_with_pli.2.txt
mv rawdata/genelists/hg19.all_genes_with_pli.2.txt rawdata/genelists/hg19.all_genes_with_pli.txt

# sHET file:
scripts/matcher.pl -file1 rawdata/genelists/hg19.genes.txt -file2 rawdata/genelists/shet.hgnc.txt -r | perl -ne 'chomp $_;  @F = split("\t", $_); print "$F[0]\t$F[5]\t$F[6]\t$F[7]\t$F[8]\t$F[9]\t$F[10]\t$F[2]\n";' > rawdata/genelists/hg19.all_genes_with_shet.txt

# Make a list of all genes and their coordinates for WES annotation:
perl -ane 'splice(@F, -2); print join("\t", @F) . "\n";' rawdata/genelists/hg19.all_genes_with_shet.txt > rawdata/genelists/hg19.coordinates.txt
perl -ane 'splice(@F, -2); print join("\t", @F) . "\n";' rawdata/genelists/hg19.all_genes_with_pli.txt >> rawdata/genelists/hg19.coordinates.txt
sort rawdata/genelists/hg19.coordinates.txt | uniq > rawdata/genelists/hg19.coordinates.2.txt
mv rawdata/genelists/hg19.coordinates.2.txt rawdata/genelists/hg19.coordinates.txt
```

### Hg38 Gene Lists

```{bash Generate hg38 Gene lists}
# Try and match genes to Hg19 based on HGNC ID
# Generate a list of hg19 genes with HGNC IDs:
perl -ane 'chomp $_; if ($F[4] ne "NA" && $F[4] ne "") {print "$F[4]\t$F[0]\t$F[5]\n";}' rawdata/genelists/hg19.genes.txt | sort | uniq > rawdata/genelists/hg19.trans.txt

scripts/matcher.pl -file1 rawdata/genelists/hg19.trans.txt -file2 rawdata/genelists/hg38.genes.txt -col2 4 -r | perl -ane 'chomp $_; print "$F[0]\t$F[1]\t$F[2]\t$F[3]\t$F[4]\t$F[5]\t$F[8]\n";' > rawdata/genelists/hg38.hgnc.matched.txt

# Ask which genes have a pLI score:
scripts/matcher.pl -file1 rawdata/genelists/hg38.hgnc.matched.txt -col1 6 -file2 rawdata/genelists/hg19.all_genes_with_pli.txt -r | perl -ane 'chomp $_; @F = split("\t", $_); print "$F[8]\t$F[9]\t$F[10]\t$F[11]\t$F[12]\t$F[13]\t$F[0]\t$F[7]\n";' > rawdata/genelists/hg38.all_genes_with_pli.txt

# Ask which genes have a sHET score:
scripts/matcher.pl -file1 rawdata/genelists/hg38.hgnc.matched.txt -col1 6 -file2 rawdata/genelists/hg19.all_genes_with_shet.txt -r | perl -ane 'chomp $_; @F = split("\t", $_); print "$F[8]\t$F[9]\t$F[10]\t$F[11]\t$F[12]\t$F[13]\t$F[0]\t$F[7]\n";' > rawdata/genelists/hg38.all_genes_with_shet.txt

# There is a fairly large caveat here, which is that I label the genes with their Hg19 ENSG ID so that I can be consistant in my R code below!!! This does't impact too many genes, they mostly have the same IDs (~2-300)
# This gets a translatable list to hg19 ENSG###:
perl -ne 'chomp $_; @F = split("\t", $_); print "$F[0]\t$F[6]\n";' rawdata/genelists/hg38.all_genes_with_pli.txt > rawdata/genelists/hg38_to_hg19_ENSG.txt
perl -ne 'chomp $_; @F = split("\t", $_); print "$F[0]\t$F[6]\n";' rawdata/genelists/hg38.all_genes_with_shet.txt >> rawdata/genelists/hg38_to_hg19_ENSG.txt
sort rawdata/genelists/hg38_to_hg19_ENSG.txt | uniq > rawdata/genelists/hg38_to_hg19_ENSG.2.txt
mv rawdata/genelists/hg38_to_hg19_ENSG.2.txt rawdata/genelists/hg38_to_hg19_ENSG.txt 

# Make a list of all genes and their coordinates for WES annotation:
perl -ane 'splice(@F, -2); print join("\t", @F) . "\n";' rawdata/genelists/hg38.all_genes_with_shet.txt > rawdata/genelists/hg38.coordinates.txt
perl -ane 'splice(@F, -2); print join("\t", @F) . "\n";' rawdata/genelists/hg38.all_genes_with_pli.txt >> rawdata/genelists/hg38.coordinates.txt
sort rawdata/genelists/hg38.coordinates.txt | uniq > rawdata/genelists/hg38.coordinates.2.txt
mv rawdata/genelists/hg38.coordinates.2.txt rawdata/genelists/hg38.coordinates.txt
```

## 4B. Running

This annotation engine requires all flags, and descriptions can also be seen by running only the `--help` flag. Briefly, these flags are:

* genelist: This is the file created above: `hg38.coordinates.txt`
* gene: An integer between 1 and the number of genes in hg38.coordinates.txt (currently 18,756 genes).
* genomeversion: The version of the genome being used. This should always be hg38 for UKBiobank data, but can be changed to Hg19 for other datasets. This takes care of two things
    + Tells which annotation set to use at `annotations/hg**/`
    + Also is a hacky way that I handle the 'chr' prefix for chromosome IDs typical of hg38 VCFs. I currently don't have a way to deal with hg38 VCFs that don't use 'chr' and, for hg19, vice versa.
* vcf: path to the UK Biobank VCF we downloaded above, with annotations at the correct locus
* samples: list of samples to include in annotations, with one ID per line.
    + This is simply the output of `bcftools query -l ukbb_plink.vcf.gz`
    + Sample IDs must match identically to those found in the header. UKBiobank WES VCF IDs look like "123456_123456", where 123456 is the eid of the individual.
    + This is important to get correct, as this is how both AF is calculated and how individuals are included in the final output.
* maf: maf filter to apply to variants. This _only_ filters based on the variants in the VCF file, not provided gnomAD MAFs. Can set higher, but will generate a *very* large file.
* outdir: Where to store per-gene txt files like: `results_ukbb_wes/ENSG00000003056.rare_variants.tsv`
* rootdir: Where are annotations we created above stored?

This command is intended to be parallelized, and each job will essentially annotated 1 gene at a time within the file given to `-genelist`. In the below code chunk I have given the raw command to run the first gene in this file, with an example LSF submission for parallelization given below.

The final result of this code chunk is what is provided as input to the PhenotypeTesting.Rmd document.

```{bash Run Annotations, eval = F}

## Example to run one gene.
java -XX:+UseSerialGC -Xms2400M -Xmx2400M -jar /nfs/ddd0/eg15/SNVCounter.jar -genelist rawdata/genelists/hg38.coordinates.txt -gene 1 -genomeversion hg38 -vcf vcfs/ukbb_plink.vcf.gz -samples ukbb_ids.txt -maf 0.001 -outdir results_ukbb_wes/ -rootdir annotations/

java -XX:+UseSerialGC -Xms2400M -Xmx2400M -jar /nfs/ddd0/eg15/SNVCounter.jar -genelist hg38.coordinates.txt -gene 1 -genomeversion hg38 -vcf vcfs/ukbb_plink.vcf.gz -samples ukbb_ids.txt -maf 0.001 -outdir results_ukbb_wes/ -rootdir annotations/

## Example job submission command for LSF:
bsub -q normal -M 2500 -o count.%J.%I -J 'SNV[1-18756]%1000' "java -XX:+UseSerialGC -Xms2400M -Xmx2400M -jar /nfs/ddd0/eg15/SNVCounter.jar -genelist annotations/hg38/genelists/hg38.coordinates.txt -gene \$LSB_JOBINDEX -genomeversion hg38 -vcf vcfs/ukbb_plink.vcf.gz -samples ukbb_ids.txt -maf 0.001 -outdir results_ukbb_wes/ -rootdir annotations/"

## Mash the genes together:
grep -hv 'NONE' results_ukbb_wes_revised/*.rare_variants.tsv > counts.ukbb_wes.txt
```
